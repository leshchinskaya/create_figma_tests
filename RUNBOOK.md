# Инструкция по Генерации Тестов

Этот документ описывает два основных рабочих процесса: создание компонентных тестов и создание сценарных тестов.

---

## Первоначальная настройка

Перед первым запуском любого из воркфлоу необходимо выполнить однократную настройку окружения.

1. **Установка зависимостей.** Запустите скрипт из корневой директории проекта:
   ```bash
   sh setup.sh
   ```
   Этот скрипт создаст виртуальное окружение (`venv`), установит все необходимые Python-пакеты из `requirements.txt` и
   сделает основные скрипты исполняемыми.

2. **Создание и настройка конфигурационного файла.** Скопируйте шаблон конфигурации в рабочий файл:
   ```bash
   cp config_template.py config.py
   ```
   После копирования откройте `config.py` и заполните как минимум следующие поля:
    ```python
    # --- Настройки Jira ---
    # URL вашей Jira
    JIRA_URL = "https://your-jira-instance.com"
    # Ключ проекта в Jira
    JIRA_PROJECT_KEY = "PROJ"
    # Ваш логин в Jira
    JIRA_USERNAME = "YOUR_JIRA_USERNAME"
    # Ваш пароль или API-токен для Jira
    JIRA_PASSWORD = "YOUR_JIRA_PASSWORD_OR_API_TOKEN"
    # Список глобальных меток, которые будут добавляться ко всем создаваемым тестам
    JIRA_LABELS = ["autotest-candidate"]
    # Тип задачи для тест-кейсов (обычно "Test")
    ISSUE_TYPE = "Test"

    # --- Настройки Figma ---
    # Ваш персональный токен доступа Figma
    FIGMA_TOKEN = "YOUR_FIGMA_PERSONAL_ACCESS_TOKEN"
    # URL файла Figma для анализа
    FIGMA_FILE_URL = "https://www.figma.com/file/your-file-id/file-name"
    
    # --- Настройки Swagger ---
    # Публичная ссылка на Swagger UI (используется для описаний в Jira)
    SWAGGER_URL = "https://example.com/swagger.yaml"
    # Локальный путь к swagger-файлу (используется для парсинга).
    SWAGGER_LOCAL_PATH = "../swagger/file.yaml"

    # --- Режим работы ---
    # "FILE_EXPORT" - генерировать CSV-файл с тестами (для первого воркфлоу).
    OPERATIONAL_MODE = "FILE_EXPORT"

    # --- Прочие настройки ---
    # Установите True, чтобы сгенерированные файлы (промпты, json) открывались автоматически
    AUTOLAUNCH_FILES = False
    ```

---

## Workflow 1: Генерация Компонентных Тестов

**Цель:** Создать детальные компонентные тесты на основе дизайна в Figma, требований и Swagger. Результат этого
воркфлоу (`component_tests.csv`) является входными данными для генерации сценарных тестов.

**Шаг 1: Получение тестов из Figma (если требуется обновление)**

* Убедитесь, что в `config.py` установлен `OPERATIONAL_MODE = "FILE_EXPORT"`.
* Запустите скрипт для анализа Figma:
  ```bash
  python3 send_figma_tests_all_tests.py
  ```
* **Результат:** В папке `create_final_tests/artifacts/` будет создан или обновлен файл `tests_from_figma.csv`.

**Шаг 2: Сбор требований через GUI**

* Запустите графический интерфейс, чтобы выбрать нужные разделы ТЗ и обновить файл `req.md`:
  ```bash
  python3 create_final_tests/folder_structure/gui_update_file_structure.py
  ```
* **Результат:** Файл `create_final_tests/artifacts/req.md` будет создан или обновлён.

**Шаг 3: Генерация промпта для AI**

* Запустите скрипт, который соберет все артефакты (требования, swagger, тесты из Figma) в один промпт.
  ```bash
  ./generate_component_prompt.sh
  ```
* **Результат:** Будет создан файл `create_final_tests/artifacts/final_prompt_component.txt`.
* Если в `config.py` установлено `AUTOLAUNCH_FILES = True`, файл промпта и `component_tests.json` откроются
  автоматически.

**Шаг 4: Работа с AI и получение JSON**

1. Скопируйте всё содержимое файла `final_prompt_component.txt` и передайте его AI. В начале файла присутствует команда
   для генерации таск-листа.
2. Проверьте полученный список задач, при необходимости скорректируйте его и утвердите финальный набор тестов и их
   содержимое.
3. Затем скажите AI сгенерировать тесты на основе утвержденного списка.
4. После этого сделайте запрос для проверки, что тесты совпадают с таск-листом - иногда AI путается и теряет.
5. Возьмите чеклист `review_component.md` и вставьте AI. Сделайте ревью предлагаемых фиксов и скажите ему поправить
   тесты.
6. Полученный от AI ответ в формате JSON сохраните в файл: `create_final_tests/artifacts/component_tests.json` и
   убедитесь, что набор тестов полный и корректный.

**Шаг 5: Отправка в Jira и/или конвертация в CSV**

* Запустите скрипт, чтобы отправить созданные компонентные тесты в Jira и сохранить CSV с созданными задачами.
  ```bash
  python3 -m create_final_tests.jira_sender --input create_final_tests/artifacts/component_tests.json --download-csv
  ```
* **Результат:** В каталоге `create_final_tests/artifacts/` появится файл `component_tests.csv`. Тесты так же загрузятся
  в Xray. Ссылка на созданные задачи появится в консоли.

---

## Workflow 2: Генерация Сценарных Тестов

**Цель:** Создать высокоуровневые end-to-end сценарии на основе уже готовых компонентных тестов, требований и Swagger.

**Обязательное условие:** У вас должен быть актуальный файл `create_final_tests/artifacts/component_tests.csv`. Он
создается на предыдущем этапе. Так же ТЗ и Swagger предполагаются что готовы после прошлого этапа.

**Шаг 1: Генерация промпта для AI**

* Запустите скрипт, который соберет все необходимые артефакты.
  ```bash
  ./generate_scenario_prompt.sh
  ```
* **Результат:** Будет создан файл `create_final_tests/artifacts/final_prompt_scenario.txt`.
* Если в `config.py` установлено `AUTOLAUNCH_FILES = True`, файл промпта и `scenario_tests.json` откроются
  автоматически.

**Шаг 2: Работа с AI и получение JSON**

1. Скопируйте всё содержимое файла `final_prompt_scenario.txt` и передайте его AI. В начале файла расположена команда
   для генерации таск-листа.
2. Проверьте полученный список задач, при необходимости скорректируйте его и утвердите финальный набор тестов и их
   содержимое.
3. Затем скажите AI сгенерировать тесты на основе утвержденного списка.
4. После этого сделайте запрос для проверки, что тесты совпадают с таск-листом - иногда AI путается и теряет.
5. Возьмите чеклист `review_scenario.md` и вставьте AI. Сделайте ревью предлагаемых фиксов и скажите ему поправить
   тесты.
6. Сохраните итоговый JSON-ответ в файл `create_final_tests/artifacts/scenario_tests.json` и убедитесь в полноте набора
   тестов.

**Шаг 3: Отправка сценарных тестов в Jira**

* Запустите скрипт для отправки тестов.
  ```bash
  python3 -m create_final_tests.jira_sender --input create_final_tests/artifacts/scenario_tests.json
  ```
* **Результат:** Сценарные тесты будут созданы в Jira. Ссылка на созданные задачи появится в консоли.
