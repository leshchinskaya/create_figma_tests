# Верхнеуровневое овервью

Данный инструмент позволяет генерировать проверки из артефактов (ТЗ, Сваггер и дизайн)
Общий флоу такой:

1. Сгенерировать проверки по дизайну в текстовом виде (можно и в Xray)
2. Сгенерировать финальные проверки полагаясь на все артефакты

Для работы рекомендуется использовать LLM модели с Chain-of-Thought возможностями.

# Порядок работы

> Более подробно логика компонентов описана в `README.md`

1. Изначальная настройка:
    1. Выполните `sh setup.sh`. Он установит python (если нужно), создаст venv и установит зависимости
    2. Скопируйте файл `config_template.py` с названием `config.py`
    3. Включите AUTOLAUNCH_FILES=True если хотите чтобы файлы промптов и тестов открывались автоматически (удобно для
       CLI).
2. Генерация тестов по фигме:
    1. В `config.py` укажите `FRAME_INCLUDE` и `OPERATIONAL_MODE = "FILE_EXPORT"`.
    2. Настройте ссылку на Figma и токен в том же файле.
    3. Запустите `send_figma_tests_all_tests.py`.
    4. Перейдите в папку `create_final_tests/artifacts` – файл `tests_from_figma.csv` будет создан автоматически.
3. Генерация единого файла ТЗ
    1. Склонируйте репозиторий с требованиями на одном уровне с этим репозиторием (например обе папки в ~).
    2. Перейдите в папку `create_final_tests/folder_structure`
    3. В `task_list_configuration.md` перечислите нужные папки с ТЗ.
        1. Если нужно - удалите ненужные файлы из этих папок (локально, пушить нельзя!)
    4. Затем выполните `update_file_structure.py`. Файл `req.md` будет обновлён автоматически.
    5. Если требования представлены в виде PDF, запустите:
       ```bash
       python3 convert_pdf_to_req.py /path/to/rigla-demo-specification/requirements.pdf
       ```
       Полученный Markdown будет записан в `req.md`.
4. Актуализация сваггера
    1. Укажите путь к актуальному swagger в `SWAGGER_LOCAL_PATH` внутри `config.py`. Файл копировать не нужно.
    2. Если сваггер станет в несколько файлов - используйте генератор file_structure для объединения в один файл.
5. Генерация промпта для AI
    1. Перейдите в `create_final_tests/artifacts`.
    2. В начале `prompt.md` актализируйте запрос (указав покрытие желаемых фич).
    3. Запустите `create_final_prompt.py`.
    4. Проверьте что `final_prompt.txt` обновился.
6. Работа с AI
    1. Содержимое `create_final_tests/artifacts/final_prompt.txt` передайте в AI.
    2. Получив ответ, попросите модель сверить его с исходным запросом и указать возможные ошибки. Обычно это сильно
       увеличивает качество финальных проверок.
       Проверьте анализ вручную на валидность, иногда нейросеть ошибается в правках.
       Используйте чеклисты в `create_final_tests/artifacts/review_component.md` для удобства.
    3. Попросите модель внести отобранные вами правки с уточнениями по необходимости и вернуть полный финальный JSON.
7. Генерация тестов
    1. Поместите JSON содержимое в `component_tests.json`.
    2. В `config.py` установите `JIRA_PROJECT_KEY` и `JIRA_LABELS` (это будут общие лейблы для всех тестов)
    3. Выполните `send_final_tests.py`.
    4. Ссылка на созданные задачи появится в выводе.
    5. После этого сделайте ревью тестов и вручную доведите их до желаемого состояния (но вполне можно общаться с чатом
       уже точечно)
