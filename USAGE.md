# Верхнеуровневое овервью

Данный инструмент позволяет генерировать проверки из артефактов (ТЗ, Сваггер и дизайн)
Общий флоу такой:

1. Сгенерировать проверки по дизайну в текстовом виде (можно и в Xray)
2. Сгенерировать финальные проверки полагаясь на все артефакты

Для работы рекомендуется использовать LLM модели с Chain-of-Thought возможностями.

# Порядок работы

> Более подробно логика компонентов описана в `README.md`

1. Изначальная настройка:
    1. Выполните `sh setup.sh`. Он установит python (если нужно), создаст venv и установит зависимости
    2. Скопируйте файл `config_template.py` с названием `config.py`
    3. Включите AUTOLAUNCH_FILES=True если хотите чтобы файлы промптов и тестов открывались автоматически (удобно для
       CLI).
2. Генерация тестов по фигме:
    1. В `config.py` укажите `FRAME_INCLUDE` и `OPERATIONAL_MODE = "FILE_EXPORT"`.
    2. Настройте ссылку на Figma и токен в том же файле.
    3. Запустите `send_figma_tests_all_tests.py`.
    4. Перейдите в папку `create_final_tests/artifacts` – файл `tests_from_figma.csv` будет создан автоматически.
3. Генерация единого файла ТЗ
    1. Склонируйте репозиторий с требованиями на одном уровне с этим репозиторием (например обе папки в ~).
    2. Перейдите в папку `create_final_tests/folder_structure`
    3. В `task_list_configuration.md` перечислите нужные папки с ТЗ.
        1. Если нужно - удалите ненужные файлы из этих папок (локально, пушить нельзя!)
    4. Затем выполните `update_file_structure.py`. Файл `req.md` будет обновлён автоматически.
    5. Если требования представлены в виде PDF или на страницах Confluence, используйте `convert_pdf_to_req.py`
       для их преобразования в локальную структуру папок с Markdown файлами:
       ```bash
       # Для локального PDF-файла:
       python3 convert_pdf_to_req.py /путь/к/вашему/файлу.pdf

       # Для PDF-файла по URL:
       python3 convert_pdf_to_req.py https://example.com/документ.pdf

       # Для страницы Confluence (включая дочерние страницы и вложения):
       python3 convert_pdf_to_req.py "https://wiki.example.com/pages/viewpage.action?pageId=12345"
       ```
       **Важно:**
       - Перед использованием для Confluence, убедитесь, что `CONFLUENCE_BASE_URL`, `CONFLUENCE_USERNAME` и `CONFLUENCE_PASSWORD` указаны в `config.py`.
       - Скрипт создаст структуру папок в `create_final_tests/folder_structure/имя_документа/`.
         - Для PDF: `page_N/content.md` для каждой страницы.
         - Для Confluence: `page_1/content.md` (содержащий всю страницу) и `page_1/attachments/` для всех вложений (изображения и т.д.). Ссылки на вложения в Markdown будут обновлены.
       - Этот скрипт не создает единый `req.md`, а предоставляет детализированную структуру для дальнейшей работы или ручной консолидации.
       - Убедитесь, что установлены зависимости: `pip install pdfminer.six markdownify requests` (обычно через `setup.sh` и `requirements.txt`).
4. Актуализация сваггера
    1. Укажите путь к актуальному swagger в `SWAGGER_LOCAL_PATH` внутри `config.py`. Файл копировать не нужно.
    2. Если сваггер станет в несколько файлов - используйте генератор file_structure для объединения в один файл.
5. Генерация промпта для AI
    1. Перейдите в `create_final_tests/artifacts`.
    2. В начале `prompt.md` актализируйте запрос (указав покрытие желаемых фич).
    3. Запустите `create_final_prompt.py`.
    4. Проверьте что `final_prompt.txt` обновился.
6. Работа с AI
    1. Содержимое `create_final_tests/artifacts/final_prompt.txt` передайте в AI.
    2. Получив ответ, попросите модель сверить его с исходным запросом и указать возможные ошибки. Обычно это сильно
       увеличивает качество финальных проверок.
       Проверьте анализ вручную на валидность, иногда нейросеть ошибается в правках.
       Используйте чеклисты в `create_final_tests/artifacts/review_component.md` для удобства.
    3. Попросите модель внести отобранные вами правки с уточнениями по необходимости и вернуть полный финальный JSON.
7. Генерация тестов
    1. Поместите JSON содержимое в `component_tests.json`.
    2. В `config.py` установите `JIRA_PROJECT_KEY` и `JIRA_LABELS` (это будут общие лейблы для всех тестов)
    3. Выполните `send_final_tests.py`.
    4. Ссылка на созданные задачи появится в выводе.
    5. После этого сделайте ревью тестов и вручную доведите их до желаемого состояния (но вполне можно общаться с чатом
       уже точечно)
